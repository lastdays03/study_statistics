{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Session 1: ë°ì´í„° ë¬¸í•´ë ¥ê³¼ ê¸°ìˆ  í†µê³„ (Integrated)\n",
                "\n",
                "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
                "- **Deep Theory**: ëª¨ì§‘ë‹¨ vs í‘œë³¸, ììœ ë„($n-1$)ì˜ ì˜ë¯¸ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤.\n",
                "- **Practice**: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´ í‰ê· ê³¼ ë¶„ì‚°ì„ ì§ì ‘ êµ¬í˜„í•˜ë©° ì›ë¦¬ë¥¼ ì²´í™”í•©ë‹ˆë‹¤.\n",
                "- **Concepts**: í‰ê·  vs ì¤‘ì•™ê°’ì˜ ì°¨ì´ì™€ ë°ì´í„° ì‹œê°í™”ì˜ ì¤‘ìš”ì„±ì„ ìµí™ë‹ˆë‹¤.\n",
                "- **Experiment**: ë¶„í¬ì˜ ëª¨ì–‘(Skewness)ì™€ ì´ìƒì¹˜ê°€ í†µê³„ëŸ‰ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤í—˜í•©ë‹ˆë‹¤.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 1. ğŸ“ Deep Theory: í†µê³„í•™ì˜ ê¸°ì´ˆì™€ ì–¸ì–´\n",
                "\n",
                "## 1. í†µê³„ì˜ ì–¸ì–´ (Notation)\n",
                "\n",
                "í†µê³„í•™ì—ì„œëŠ” **'ì „ì²´(Population)'**ì™€ **'ì¼ë¶€ë¶„(Sample)'**ì„ ì—„ê²©í•˜ê²Œ êµ¬ë¶„í•˜ë©°, ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê¸°í˜¸ë„ ë‹¤ë¦…ë‹ˆë‹¤.\n",
                "\n",
                "| ê°œë… | ëª¨ì§‘ë‹¨ (Population) | í‘œë³¸ (Sample) |\n",
                "| :--- | :---: | :---: |\n",
                "| **ê´€ì‹¬ ëŒ€ìƒ** | ì‹ ì˜ ì˜ì—­ (ì „ì²´ ë°ì´í„°) | ì¸ê°„ì˜ ì˜ì—­ (ê´€ì¸¡ëœ ë°ì´í„°) |\n",
                "| **ê°œìˆ˜ (Size)** | $N$ | $n$ |\n",
                "| **í‰ê·  (Mean)** | $\\mu$ (ë®¤) | $\\bar{x}$ (ì—‘ìŠ¤ ë°”) |\n",
                "| **ë¶„ì‚° (Variance)** | $\\sigma^2$ (ì‹œê·¸ë§ˆ ì œê³±) | $s^2$ (ì—ìŠ¤ ì œê³±) |\n",
                "| **í‘œì¤€í¸ì°¨ (Std Dev)** | $\\sigma$ | $s$ |\n",
                "\n",
                "> **Why?**: ìš°ë¦¬ëŠ” $\\mu$ë¥¼ ì•Œ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, $\\bar{x}$ë¥¼ í†µí•´ $\\mu$ë¥¼ **ì¶”ì •(Estimate)**í•©ë‹ˆë‹¤.\n",
                "\n",
                "## 2. í•©ì˜ ê¸°í˜¸ ì‹œê·¸ë§ˆ ($\\sum$)\n",
                "\n",
                "ë°ì´í„° $x = \\{x_1, x_2, ..., x_n\\}$ê°€ ìˆì„ ë•Œ, ëª¨ë“  ë°ì´í„°ì˜ í•©ì€ ë‹¤ìŒê³¼ ê°™ì´ ì”ë‹ˆë‹¤.\n",
                "\n",
                "$$ \\sum_{i=1}^{n} x_i = x_1 + x_2 + ... + x_n $$\n",
                "\n",
                "**ì„±ì§ˆ**:\n",
                "1.  $\\sum_{i=1}^{n} c = nc$ (ìƒìˆ˜ $c$ë¥¼ $n$ë²ˆ ë”í•¨)\n",
                "2.  $\\sum_{i=1}^{n} cx_i = c \\sum_{i=1}^{n} x_i$ (ìƒìˆ˜ëŠ” ë°–ìœ¼ë¡œ ëº„ ìˆ˜ ìˆìŒ)\n",
                "3.  $\\sum (x_i + y_i) = \\sum x_i + \\sum y_i$\n",
                "\n",
                "## 3. í¸ì°¨ì˜ í•©ì€ 0ì´ë‹¤ (ì¦ëª…)\n",
                "\n",
                "**í¸ì°¨(Deviation)**ëŠ” ë°ì´í„°ê°€ í‰ê· ìœ¼ë¡œë¶€í„° ë–¨ì–´ì§„ ê±°ë¦¬($x_i - \\bar{x}$)ì…ë‹ˆë‹¤. í¸ì°¨ì˜ ì´í•©ì€ í•­ìƒ 0ì…ë‹ˆë‹¤.\n",
                "\n",
                "$$ \\sum_{i=1}^{n} (x_i - \\bar{x}) = \\sum x_i - \\sum \\bar{x} $$\n",
                "$$ = \\sum x_i - n\\bar{x} $$\n",
                "$$ = \\sum x_i - n(\\frac{\\sum x_i}{n}) $$\n",
                "$$ = \\sum x_i - \\sum x_i = 0 $$\n",
                "\n",
                "> **Insight**: ê·¸ë˜ì„œ 'í‰ê· ì ì¸ ê±°ë¦¬'ë¥¼ êµ¬í•  ë•Œ ë‹¨ìˆœ í•©ì„ í•˜ë©´ 0ì´ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ **ì œê³±(Square)**ì„ í•˜ê±°ë‚˜ **ì ˆëŒ“ê°’(Absolute)**ì„ ì”Œì›ë‹ˆë‹¤. -> ì´ê²ƒì´ **ë¶„ì‚°**ê³¼ **ì ˆëŒ€í¸ì°¨**ì˜ ì‹œì‘ì…ë‹ˆë‹¤.\n",
                "\n",
                "## 4. ììœ ë„ (Degrees of Freedom): ì™œ $n-1$ì¸ê°€?\n",
                "\n",
                "í‘œë³¸ ë¶„ì‚°($s^2$)ì„ êµ¬í•  ë•Œ, $n$ì´ ì•„ë‹ˆë¼ $n-1$ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\n",
                "\n",
                "$$ s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $$\n",
                "\n",
                "### ì§ê´€ì  ì´í•´ (Intuition)\n",
                "- í‘œë³¸ ë¶„ì‚°ì€ ëª¨ë¶„ì‚°($\\sigma^2$)ì„ ì¶”ì •í•˜ê¸° ìœ„í•œ ë„êµ¬ì…ë‹ˆë‹¤.\n",
                "- ê·¸ëŸ°ë° í‘œë³¸ì€ ëª¨ì§‘ë‹¨ë³´ë‹¤ ë³´í†µ **ëœ í¼ì ¸ìˆìŠµë‹ˆë‹¤**. (ì¤‘ì‹¬ ê·¹í•œ ì •ë¦¬ ë“±ì— ì˜í•´ ê·¹ë‹¨ê°’ì´ ì ê²Œ ë½‘í í™•ë¥ ì´ ë†’ìŒ)\n",
                "- ë‹¨ìˆœíˆ $n$ìœ¼ë¡œ ë‚˜ëˆ„ë©´ ëª¨ë¶„ì‚°ë³´ë‹¤ **ì‘ê²Œ ì¶”ì •(Underestimation)**ë˜ëŠ” í¸í–¥(Bias)ì´ ë°œìƒí•©ë‹ˆë‹¤.\n",
                "- ì´ë¥¼ ë³´ì •í•˜ê¸° ìœ„í•´ ë¶„ëª¨ë¥¼ $n$ë³´ë‹¤ ì‘ì€ $n-1$ë¡œ ì¤„ì—¬ì„œ, ê°’ì„ ì•½ê°„ í‚¤ì›Œì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. (**Bessel's Correction**)\n",
                "\n",
                "### ììœ ë„ ê´€ì \n",
                "- $n$ê°œì˜ ë°ì´í„°ê°€ ìˆì„ ë•Œ, í‰ê·  $\\bar{x}$ê°€ ì •í•´ì§€ë©´, $n-1$ê°œì˜ ë°ì´í„°ëŠ” ììœ ë¡­ê²Œ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆì§€ë§Œ, **ë§ˆì§€ë§‰ 1ê°œëŠ” ìë™ìœ¼ë¡œ ì •í•´ì§‘ë‹ˆë‹¤**. (í¸ì°¨ì˜ í•©ì´ 0ì´ì–´ì•¼ í•˜ë¯€ë¡œ)\n",
                "- ë…ë¦½ì ì¸ ì •ë³´ì˜ ê°œìˆ˜ëŠ” $n$ê°œê°€ ì•„ë‹ˆë¼ $n-1$ê°œì…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 2. ğŸ’» Manual Implementation: ë°‘ë°”ë‹¥ë¶€í„° ë§Œë“œëŠ” í†µê³„\n",
                "\n",
                "`numpy` ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´, íŒŒì´ì¬ì˜ ê¸°ë³¸ ê¸°ëŠ¥(`sum`, `len`, `**`)ë§Œìœ¼ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ ì§ì ‘ êµ¬í˜„í•´ ë´…ë‹ˆë‹¤.\n",
                "ìœ„ì—ì„œ ë°°ìš´ ê³µì‹ì„ ì½”ë“œë¡œ ì˜®ê¸°ë©° ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì²´í™”í•©ë‹ˆë‹¤.\n",
                "\n",
                "### ë¯¸ì…˜: `my_variance` í•¨ìˆ˜ ë§Œë“¤ê¸°\n",
                "$$ s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2 $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def my_mean(data):\n",
                "    # TODO: sum()ê³¼ len()ë§Œ ì‚¬ìš©í•˜ì—¬ í‰ê·  êµ¬í˜„\n",
                "    return sum(data) / len(data)\n",
                "\n",
                "def my_variance(data, ddof=1):\n",
                "    # TODO: ë¶„ì‚° ê³µì‹ êµ¬í˜„ (ddof=1ì€ n-1ë¡œ ë‚˜ëˆ”ì„ ì˜ë¯¸)\n",
                "    m = my_mean(data)\n",
                "    # í¸ì°¨ì˜ ì œê³±ì˜ í•©\n",
                "    squared_diff_sum = sum([(x - m)**2 for x in data])\n",
                "    \n",
                "    # ììœ ë„ ì ìš© (n - ddof)\n",
                "    return squared_diff_sum / (len(data) - ddof)\n",
                "\n",
                "# ê²€ì¦ ì½”ë“œ\n",
                "test_data = [1, 2, 3, 4, 5, 100]\n",
                "\n",
                "print(f\"My Mean: {my_mean(test_data):.2f} vs Numpy: {np.mean(test_data):.2f}\")\n",
                "print(f\"My Var(n-1): {my_variance(test_data, ddof=1):.2f} vs Numpy: {np.var(test_data, ddof=1):.2f}\")\n",
                "print(f\"My Var(n): {my_variance(test_data, ddof=0):.2f} vs Numpy: {np.var(test_data, ddof=0):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# âš–ï¸ Preprocessing: ë°ì´í„°ì˜ ëˆˆë†’ì´ ë§ì¶”ê¸° (Scaling)\n",
                "\n",
                "ë°ì´í„°ì˜ **ë‹¨ìœ„(Scale)**ê°€ ë‹¤ë¥´ë©´ AIëŠ” í˜¼ë€ìŠ¤ëŸ¬ì›Œí•©ë‹ˆë‹¤.\n",
                "ì˜ˆë¥¼ ë“¤ì–´, í‚¤($170$)ì™€ ì‹œë ¥($1.0$)ì„ ê·¸ëŒ€ë¡œ ê³„ì‚°í•˜ë©´, ìˆ«ìê°€ í° 'í‚¤'ê°€ 'ì‹œë ¥'ì„ ì••ë„í•´ë²„ë¦½ë‹ˆë‹¤.\n",
                "ëª¨ë“  ë°ì´í„°ë¥¼ ê³µí‰í•œ ê¸°ì¤€(ëˆˆë†’ì´)ìœ¼ë¡œ ë§ì¶°ì£¼ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
                "\n",
                "### 1. í¸ì°¨ (Deviation): ëª¨ë“  í†µê³„ì˜ ì‹œì‘\n",
                "í‘œì¤€í™”ë‚˜ ì •ê·œí™”ë¥¼ í•˜ë ¤ë©´ ë¨¼ì € **'í¸ì°¨'**ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "$$ Deviation = x - \\mu $$\n",
                "\n",
                "- ë‚´ ì ìˆ˜ê°€ í‰ê· ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë†’ì€ê°€(ì–‘ìˆ˜) í˜¹ì€ ë‚®ì€ê°€(ìŒìˆ˜)?\n",
                "- ë¶„ì‚°, í‘œì¤€í¸ì°¨, ê³µë¶„ì‚°ì€ ëª¨ë‘ ì´ 'í¸ì°¨'ë¥¼ ê°€ì§€ê³  ê³„ì‚°í•©ë‹ˆë‹¤.\n",
                "\n",
                "### 2. í‘œì¤€í™” (Standardization, Z-score)\n",
                "í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë§ì¶¥ë‹ˆë‹¤. ë°ì´í„°ê°€ **\"í‰ê· ì—ì„œ ëª‡ í‘œì¤€í¸ì°¨ë§Œí¼ ë–¨ì–´ì ¸ ìˆë‚˜\"**ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
                "\n",
                "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
                "\n",
                "### 3. ì •ê·œí™” (Min-Max Normalization)\n",
                "ë°ì´í„°ë¥¼ 0ê³¼ 1 ì‚¬ì´($0 \\sim 1$)ë¡œ ì••ì¶•í•©ë‹ˆë‹¤.\n",
                "\n",
                "$$ x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}} $$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ê°€ìƒì˜ í‚¤(cm) ë°ì´í„°: í‰ê·  170, í‘œì¤€í¸ì°¨ 10\n",
                "heights = np.array([150, 160, 170, 180, 190])\n",
                "\n",
                "mean_val = np.mean(heights)\n",
                "std_val = np.std(heights)\n",
                "\n",
                "# 1. í¸ì°¨ (Deviation)\n",
                "deviations = heights - mean_val\n",
                "print(f\"Deviations: {deviations}\") # [-20 -10  0 10 20]\n",
                "\n",
                "# 2. í‘œì¤€í™” (Z-score)\n",
                "z_scores = deviations / std_val\n",
                "print(f\"Z-scores: {z_scores}\") # [-2.0 -1.0  0.0  1.0  2.0]\n",
                "# í•´ì„: 190cmëŠ” í‰ê· ë³´ë‹¤ 2ë°° í‘œì¤€í¸ì°¨ë§Œí¼ í½ë‹ˆë‹¤.\n",
                "\n",
                "# 3. ì •ê·œí™” (Min-Max)\n",
                "min_val = np.min(heights)\n",
                "max_val = np.max(heights)\n",
                "min_max_scaled = (heights - min_val) / (max_val - min_val)\n",
                "print(f\"Min-Max (0~1): {min_max_scaled}\")\n",
                "\n",
                "# ğŸš€ Practical Usage: Sklearn\n",
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
                "\n",
                "# 2ì°¨ì› ë°°ì—´ì´ì–´ì•¼ í•¨ (N, 1)\n",
                "h_reshape = heights.reshape(-1, 1)\n",
                "\n",
                "scaler = StandardScaler()\n",
                "print(f\"Sklearn Standard: {scaler.fit_transform(h_reshape).flatten()}\")\n",
                "\n",
                "scaler_mm = MinMaxScaler()\n",
                "print(f\"Sklearn MinMax: {scaler_mm.fit_transform(h_reshape).flatten()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# ğŸš€ Practical Usage: Pandas Ecosystem\n",
                "\n",
                "ì†ìœ¼ë¡œ êµ¬í˜„í•´ë³´ì•˜ìœ¼ë‹ˆ, ì‹¤ì „ì—ì„œ ì‚¬ìš©í•˜ëŠ” `Pandas`ì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì•Œì•„ë´…ì‹œë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
                "df = pd.DataFrame({\n",
                "    'length': [10, 20, 30, 40, 50, 1000],\n",
                "    'score': [1.2, 3.4, 2.1, 5.0, 4.3, 1.1]\n",
                "})\n",
                "\n",
                "# 1. í•œ ë°©ì— í†µê³„ëŸ‰ ë³´ê¸°\n",
                "print(\"=== Describe ===\")\n",
                "print(df.describe()) # í‰ê· , í‘œì¤€í¸ì°¨, 4ë¶„ìœ„ìˆ˜ê¹Œì§€ í•œë²ˆì—!\n",
                "\n",
                "# 2. ê°œë³„ í†µê³„ëŸ‰\n",
                "print(\"\\n=== Individual ===\")\n",
                "print(f\"Mean: {df['length'].mean()}\")\n",
                "print(f\"Var (unbiased): {df['length'].var()}\") # PandasëŠ” ê¸°ë³¸ì´ n-1 (ddof=1) ì…ë‹ˆë‹¤.\n",
                "\n",
                "# 3. ë¶„ìœ„ìˆ˜ (Quantile)\n",
                "q1 = df['length'].quantile(0.25)\n",
                "q3 = df['length'].quantile(0.75)\n",
                "print(f\"\\nIQR: {q3 - q1}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Part 3. ğŸ“– Concepts & Visualization: ëŒ€í‘¯ê°’ì˜ í•¨ì •\n",
                "\n",
                "## 1. ëŒ€í‘¯ê°’ì˜ í•¨ì •: í‰ê· (Mean) vs ì¤‘ì•™ê°’(Median)\n",
                "\n",
                "### ì •ì˜\n",
                "- **í‰ê·  (Mean)**: ëª¨ë“  ë°ì´í„°ì˜ í•©ì„ ê°œìˆ˜ë¡œ ë‚˜ëˆˆ ê°’. ($\\mu = \\frac{\\sum x}{N}$)\n",
                "- **ì¤‘ì•™ê°’ (Median)**: ë°ì´í„°ë¥¼ í¬ê¸° ìˆœì„œëŒ€ë¡œ ë‚˜ì—´í–ˆì„ ë•Œ ê°€ì¥ ê°€ìš´ë° ìˆëŠ” ê°’.\n",
                "\n",
                "### ì™œ ì¤‘ìš”í•œê°€? (RAG Context)\n",
                "RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œ **ì²­í¬ í¬ê¸°(Chunk Size)**ë¥¼ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤. \"ìš°ë¦¬ ë¬¸ì„œë“¤ì˜ í‰ê·  ê¸¸ì´ëŠ” 500ìë‹ˆê¹Œ ì²­í¬ í¬ê¸°ë¥¼ 500ìœ¼ë¡œ í•˜ì\"ë¼ê³  ê²°ì •í•˜ë©´ ìœ„í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "\n",
                "- **í‰ê· ì˜ ì·¨ì•½ì **: ì´ìƒì¹˜(Outlier)ì— ë§¤ìš° ë¯¼ê°í•©ë‹ˆë‹¤. ì—„ì²­ë‚˜ê²Œ ê¸´ ë¬¸ì„œ í•˜ë‚˜ê°€ ì „ì²´ í‰ê· ì„ ëŒì–´ì˜¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "- **ì¤‘ì•™ê°’ì˜ ê°•ì **: ì´ìƒì¹˜ì— ê°•ê±´(Robust)í•©ë‹ˆë‹¤.\n",
                "\n",
                "> **Insight**: ë°ì´í„° ë¶„í¬ê°€ 'ì¢… ëª¨ì–‘(Normal Distribution)'ì´ ì•„ë‹ˆë¼ë©´, í‰ê· ë³´ë‹¤ëŠ” ì¤‘ì•™ê°’ì´ 'ë³´í†µì˜ ë°ì´í„°'ë¥¼ ë” ì˜ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
                "\n",
                "## 2. ë°ì´í„°ì˜ í¼ì§(Spread): ë¶„ì‚°ê³¼ í‘œì¤€í¸ì°¨\n",
                "\n",
                "### ì •ì˜\n",
                "- **ë¶„ì‚° (Variance, $\\sigma^2$)**: ë°ì´í„°ê°€ í‰ê· ìœ¼ë¡œë¶€í„° ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€(í¸ì°¨)ë¥¼ ì œê³±í•˜ì—¬ í‰ê·  ë‚¸ ê°’.\n",
                "- **í‘œì¤€í¸ì°¨ (Standard Deviation, $\\sigma$)**: ë¶„ì‚°ì˜ ì œê³±ê·¼. ì›ë˜ ë°ì´í„°ì™€ ë‹¨ìœ„ê°€ ê°™ì•„ í•´ì„í•˜ê¸° í¸í•¨.\n",
                "\n",
                "### ì‹¤ì „ í™œìš©\n",
                "\"ëŒ€ë¶€ë¶„ì˜ ì²­í¬ê°€ 300ìì—ì„œ 700ì ì‚¬ì´ì— ëª¨ì—¬ìˆëŠ”ê°€?\"ë¥¼ ì•Œê¸° ìœ„í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
                "- **ì •ê·œë¶„í¬ ê°€ì • ì‹œ**: í‰ê·  $\\pm$ 2$\\sigma$ ë²”ìœ„ ë‚´ì— ì•½ 95%ì˜ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
                "- **3-Sigma Rule**: í‰ê·  $\\pm$ 3$\\sigma$ë¥¼ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°ë¥¼ ì´ìƒì¹˜ë¡œ ê·œì •í•˜ê³  ì œê±°í•˜ê±°ë‚˜ ì˜ë¼ë‚¼(Truncate) ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
                "\n",
                "## 3. ë¶„í¬ì˜ ëª¨ì–‘ì„ ë³´ëŠ” ë²•: ì‹œê°í™”\n",
                "\n",
                "ìˆ«ìë§Œ ë´ì„œëŠ” ë°ì´í„°ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°˜ë“œì‹œ ê·¸ë ¤ë´ì•¼ í•©ë‹ˆë‹¤.\n",
                "\n",
                "### íˆìŠ¤í† ê·¸ë¨ (Histogram)\n",
                "ë°ì´í„°ê°€ ëª°ë ¤ìˆëŠ” êµ¬ê°„ì„ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.\n",
                "- **Right Skewed**: ì™¼ìª½ìœ¼ë¡œ ì¹˜ìš°ì¹˜ê³  ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ê°€ ê¸´ í˜•íƒœ. (ëŒ€ë¶€ë¶„ì˜ í…ìŠ¤íŠ¸ ë°ì´í„° ê¸¸ì´ ë¶„í¬)\n",
                "- ì´ ê²½ìš° **í‰ê·  > ì¤‘ì•™ê°’**ì¸ ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.\n",
                "\n",
                "### ë°•ìŠ¤í”Œë¡¯ (Box Plot)\n",
                "ë°ì´í„°ì˜ 4ë¶„ìœ„ìˆ˜(Quartile)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„í¬ì™€ ì´ìƒì¹˜ë¥¼ í•œëˆˆì— ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
                "- **IQR (Interquartile Range)**: ìƒìœ„ 25%(Q3)ì™€ í•˜ìœ„ 25%(Q1)ì˜ ì°¨ì´. ë°ì´í„°ì˜ ì¤‘ê°„ 50%ê°€ ëª¨ì¸ êµ¬ê°„.\n",
                "- **Outlier íŒë³„**: ë³´í†µ $Q3 + 1.5 \\times IQR$ ë³´ë‹¤ í° ê°’ì„ ì´ìƒì¹˜ë¡œ ë´…ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# ì‹œê°í™” ìŠ¤íƒ€ì¼ ì„¤ì •\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['font.family'] = 'AppleGothic'\n",
                "plt.rcParams['axes.unicode_minus'] = False\n",
                "\n",
                "np.random.seed(42)\n",
                "\n",
                "# ì •ìƒì ì¸ ë¬¸ì¥ ê¸¸ì´ ë°ì´í„° (í‰ê·  50, í‘œì¤€í¸ì°¨ 10)\n",
                "normal_data = np.random.normal(50, 10, 100)\n",
                "normal_data = np.maximum(normal_data, 5) # ìŒìˆ˜ ë°©ì§€\n",
                "\n",
                "print(f\"[Normal] Mean: {np.mean(normal_data):.2f}\")\n",
                "print(f\"[Normal] Median: {np.median(normal_data):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. ë¶„í¬ì˜ ì¹˜ìš°ì¹¨(Skewness)ê³¼ ë¾°ì¡±í•¨(Kurtosis)\n",
                "\n",
                "- **ì™œë„ (Skewness)**: 0ì´ë©´ ëŒ€ì¹­. ì–‘ìˆ˜ë©´ ì™¼ìª½ìœ¼ë¡œ ì¹˜ìš°ì¹¨(ì˜¤ë¥¸ìª½ ê¼¬ë¦¬). \n",
                "- **ì²¨ë„ (Kurtosis)**: 0(ë˜ëŠ” 3)ì´ë©´ ì •ê·œë¶„í¬. í´ìˆ˜ë¡ ë¾°ì¡±í•¨(ì´ìƒì¹˜ê°€ ë§ìŒ).\n",
                "\n",
                "PandasëŠ” ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Skewness: {pd.Series(normal_data).skew():.4f}\") # ì •ê·œë¶„í¬ë¼ 0ì— ê°€ê¹Œì›€\n",
                "print(f\"Kurtosis: {pd.Series(normal_data).kurt():.4f}\") # ì •ê·œë¶„í¬ë¼ 0ì— ê°€ê¹Œì›€ (Fisher's definition)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### ğŸ”¥ Tweak & Break: ì´ìƒì¹˜ ì¶”ê°€ ì‹¤í—˜\n",
                "ë°ì´í„°ì— **ê·¹ë‹¨ì ì¸ ê°’(Outlier)** í•˜ë‚˜ê°€ ë“¤ì–´ì˜¤ë©´ í‰ê· ê³¼ ì¤‘ì•™ê°’ì€ ì–´ë–»ê²Œ ë³€í• ê¹Œìš”?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ê¸¸ì´ 10,000ìì¸ ì—„ì²­ë‚œ ê¸¸ì´ì˜ ë¬¸ì„œ í•˜ë‚˜ê°€ ì¶”ê°€ë¨\n",
                "outlier_data = np.append(normal_data, [10000])\n",
                "\n",
                "mean_val = np.mean(outlier_data)\n",
                "median_val = np.median(outlier_data)\n",
                "\n",
                "print(f\"[With Outlier] Mean: {mean_val:.2f} (ë³€í™”ëŸ‰: {mean_val - np.mean(normal_data):.2f})\")\n",
                "print(f\"[With Outlier] Median: {median_val:.2f} (ë³€í™”ëŸ‰: {median_val - np.median(normal_data):.2f})\")\n",
                "\n",
                "# ì‹œê°í™”ë¡œ í™•ì¸\n",
                "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
                "sns.histplot(normal_data, ax=ax[0], kde=True).set_title(\"Normal Data\")\n",
                "sns.histplot(outlier_data, ax=ax[1], kde=True).set_title(\"With Outlier (10000)\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **Insight**: í‰ê· ì€ ì´ìƒì¹˜ í•˜ë‚˜ì— ì˜í•´ ê¸‰ì¦í•˜ì§€ë§Œ, ì¤‘ì•™ê°’ì€ ê±°ì˜ ë³€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.  \n",
                "> **RAG Tip**: ë¬¸ì„œ ê¸¸ì´ì˜ ëŒ€í‘¯ê°’ì„ ì¡ì„ ë•Œ, ì´ìƒì¹˜ê°€ ë§ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¼ë©´ **ì¤‘ì•™ê°’**ì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Part 4. ğŸ§ª Experiment & Verify: ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í—˜\n",
                "\n",
                "### Scenario A: ìƒ˜í”Œ ìˆ˜(N)ì— ë”°ë¥¸ í‰ê· ì˜ ì‹ ë¢°ë„\n",
                "ë°ì´í„°ê°€ ì ì„ ë•Œ(Data Scarcity) êµ¬í•œ í‰ê· ì„ ë¯¿ì„ ìˆ˜ ìˆì„ê¹Œìš”?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# ëª¨ì§‘ë‹¨: í‰ê·  50ì¸ ê±°ëŒ€í•œ ë°ì´í„°ì…‹\n",
                "population = np.random.normal(50, 15, 100000)\n",
                "\n",
                "sample_sizes = [10, 100, 1000, 10000]\n",
                "results = []\n",
                "\n",
                "for n in sample_sizes:\n",
                "    sample = np.random.choice(population, n)\n",
                "    results.append({\n",
                "        'N': n,\n",
                "        'Mean': np.mean(sample),\n",
                "        'Std': np.std(sample),\n",
                "        'Error': abs(50 - np.mean(sample)) # ì‹¤ì œ í‰ê· (50)ê³¼ì˜ ì°¨ì´\n",
                "    })\n",
                "\n",
                "pd.DataFrame(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scenario B: ë¡œê·¸ ì •ê·œë¶„í¬(Log-Normal)ì™€ 3-Sigmaì˜ í•¨ì •\n",
                "ëŒ€ë¶€ë¶„ì˜ ìì—°ì–´ ë°ì´í„°(ê¸€ì ìˆ˜ ë“±)ëŠ” ì •ê·œë¶„í¬ê°€ ì•„ë‹™ë‹ˆë‹¤.  \n",
                "ì™¼ìª½ìœ¼ë¡œ ì ë ¤ìˆê³  ì˜¤ë¥¸ìª½ ê¼¬ë¦¬ê°€ ê¸´ **Log-Normal** ë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤.  \n",
                "ì´ë•Œë„ `Mean + 3 * Std` (3-Sigma Rule)ë¡œ ì´ìƒì¹˜ë¥¼ ìë¥´ëŠ” ê²Œ ë§ì„ê¹Œìš”?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ë¡œê·¸ ì •ê·œë¶„í¬ ë°ì´í„° ìƒì„±\n",
                "log_normal_data = np.random.lognormal(mean=3, sigma=1, size=1000)\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "sns.histplot(log_normal_data, kde=True)\n",
                "plt.title(\"Log-Normal Distribution (Text Length Simulation)\")\n",
                "plt.axvline(np.mean(log_normal_data), color='red', linestyle='--', label='Mean')\n",
                "plt.axvline(np.median(log_normal_data), color='green', linestyle='-', label='Median')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "mean_val = np.mean(log_normal_data)\n",
                "std_val = np.std(log_normal_data)\n",
                "\n",
                "# 3-Sigma ê¸°ì¤€ì„ \n",
                "threshold = mean_val + 3 * std_val\n",
                "outliers = log_normal_data[log_normal_data > threshold]\n",
                "\n",
                "print(f\"í‰ê· : {mean_val:.2f}, í‘œì¤€í¸ì°¨: {std_val:.2f}\")\n",
                "print(f\"3-Sigma ì„ê³„ê°’: {threshold:.2f}\")\n",
                "print(f\"ì „ì²´ ë°ì´í„° ì¤‘ ì´ìƒì¹˜ ë¹„ìœ¨: {len(outliers) / len(log_normal_data) * 100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "> **Discussion**: ë¡œê·¸ ì •ê·œë¶„í¬ì²˜ëŸ¼ ê¼¬ë¦¬ê°€ ê¸´ ë¶„í¬ì—ì„œëŠ” í‘œì¤€í¸ì°¨(Sigma)ê°€ ë§¤ìš° í¬ê²Œ ì¡í™ë‹ˆë‹¤. ê·¸ë˜ì„œ `3-Sigma`ë¥¼ ì ìš©í•˜ë©´ ì„ê³„ê°’(Threshold)ì´ ë„ˆë¬´ ë†’ì•„ì ¸ì„œ, ì‹¤ì œë¡œëŠ” ì˜ë¼ë‚´ì•¼ í•  ê¸´ ë¬¸ì„œë“¤ì„ ê±°ì˜ ê±¸ëŸ¬ë‚´ì§€ ëª»í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. \n",
                "> -> **ëŒ€ì•ˆ**: 4ë¶„ìœ„ìˆ˜ ê¸°ë°˜ì˜ **IQR ë°©ì‹**ì´ë‚˜, ìƒìœ„ 95% ë°±ë¶„ìœ„ìˆ˜(Percentile)ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "--- \n",
                "## ğŸ“ Retrospective\n",
                "ì´ ì…€ì„ ë”ë¸”í´ë¦­í•˜ì—¬ ì˜¤ëŠ˜ ë°°ìš´ ë‚´ìš©ê³¼ ëŠë‚€ ì ì„ ê¸°ë¡í•´ ë³´ì„¸ìš”.\n",
                "\n",
                "**1. Mathematical Insight**\n",
                "- ì˜ˆ: ë¶„ì‚° ì‹ì—ì„œ n-1ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ ê°€ 'ììœ ë„' ë•Œë¬¸ì´ë¼ëŠ” ê²ƒì„ ì•Œê²Œ ë¨.\n",
                "\n",
                "**2. Key Takeaways**\n",
                "- ì˜ˆ: ì§ì ‘ mean, varianceë¥¼ êµ¬í˜„í•´ë³´ë‹ˆ ì´í•´ê°€ ë” ì˜ ë¨.\n",
                "-"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}